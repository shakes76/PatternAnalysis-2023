from dataset import get_image_slices, get_image_slices_with_table
from train import train_vqvae, train_pixelcnn
from predict import visualize_reconstructions, visualize_samples, visualize_pixelcnn_generation_batch, compare_original_and_generated
from modules import VQVAE, VQVAETrainer, PixelCNN
import torch

def main():

    # Lists to store loss values and perplexities for visualization
    recon_losses = []
    vq_losses = []
    perplexities = []

    # Load the brain slices images
    train_images, test_images, _ = get_image_slices()
    # Create a dataset and data loader using the train images
    dataset = BrainSlicesDataset(train_images)
    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

    # Create a batch of test images for visualization purposes
    test_samples_for_viz = torch.stack([test_images[i].unsqueeze(0) for i in range(3)]).to(device)

    # Initialize the VQ-VAE model and move it to the appropriate device (GPU or CPU)
    vqvae_model = VQVAE(input_channels=1, hidden_channels=128, num_embeddings=512, embedding_dim=32).to(device)
    optimizer = torch.optim.Adam(vqvae_model.parameters(), lr=0.001)

    # Initialize the VQVAE trainer model and move it to the appropriate device
    vqvae = VQVAETrainer(train_images.var(), 1, 128, 512, 32).to(device)
    # Train the VQVAE model
    train_vqvae(vqvae, train_loader, num_epochs=20, learning_rate=0.0001, test_samples=test_samples_for_viz, recon_losses=recon_losses, vq_losses=vq_losses, perplexities=perplexities)

    # Initialize the PixelCNN model and move it to the appropriate device
    pixelcnn = PixelCNN((1, 128, 128), 256, 10).to(device)
    # Train the PixelCNN model
    train_pixelcnn(pixelcnn, train_loader, num_epochs=40, learning_rate=0.001)

    # Generate images using the trained PixelCNN
    with torch.no_grad():
        pixelcnn_generated_samples = torch.zeros(3, 1, 128, 128).to(device)  # batch of 3 empty images
        for i in range(128):
            for j in range(128):
                out = pixelcnn(pixelcnn_generated_samples)
                probs = F.softmax(out[:, :, i, j], dim=1)
                for b in range(3):  # For each image in the batch
                    pixelcnn_generated_samples[b, :, i, j] = torch.multinomial(probs[b], 1).float() / 255.0
        # Visualize the images generated by the PixelCNN
        visualize_samples(pixelcnn_generated_samples)

    # Visualization of reconstructions using the VQ-VAE model
    with torch.no_grad():
        # Get some test images for reconstruction visualization
        test_samples = torch.stack([test_images[i] for i in range(3)]).to(device)
        reconstructions, _, _ = vqvae(test_samples)
        # Visualize the reconstructions
        visualize_reconstructions(test_samples, reconstructions)

        # Visualize multiple images generated by the PixelCNN
        visualize_pixelcnn_generation_batch(pixelcnn, batch_size=5)

        # Compare an original image with an image generated by the PixelCNN
        for i in range(3):  # For 3 examples
            compare_original_and_generated(test_samples[i], pixelcnn)
    return recon_losses, vq_losses, perplexities

    # Print the recorded losses and perplexities
    print("Reconstruction Losses:", recon_losses)
    print("VQ Losses:", vq_losses)
    print("Perplexities:", perplexities)

if __name__ == "__main__":
     main()