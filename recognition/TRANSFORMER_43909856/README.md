# ViT Transformer for image classification of the ADNI dataset

 
## Description
This code can be used to train, validate, and test a ViT (Visual Transformer) model
on the ADNI brain dataset. The ViT model is a binary classifier that takes 2D MRI slice
images as its inputs. It attempts to determine if the patient in the MRI image slice has
either Alzheimer's disease, or a healthy (Cognitive Normal) brain.


## Dependencies
This code is written in Python 3.11.5. 

The following libraries/modules are also used:
- pytorch 2.1.0
- pytorch-cuda 11.8
- torchvision 0.16.0
- torchdata 0.7.0
- matplotlib 3.7.2
- scikit-learn 1.3.0
- einops 0.7.0

It is strongly recommended that these packages are installed within a new conda
(Anaconda/Miniconda) environment, and that the code is run within this environment. 
These libraries can then be installed into the conda environment 
using these lines in the terminal:

```
conda install pytorch torchvision torchaudio torchdata pytorch-cuda=11.8 -c pytorch -c nvidia

conda install matplotlib

conda install scikit-learn

pip install einops
``````

### Reproducibility
Model training was completed on the UQ Rangpur HPC server, using the vgpu40
node with the following hardware specifications:

- 8x vCPU cores (AMD Zen 2)
- 128 GB RAM
- NVIDIA A100 40GB vGPU

For more information, see [UQ EAIT compute infrastructure.](https://student.eait.uq.edu.au/infrastructure/compute/)

The model was saved after training, and inference was later completed on a local device.


## How a ViT works
A ViT (Visual Transformer) is a variation on Transformer models, designed specifically for image processing.

In a Transformer, input data and target values are given an embedding with a positional encoding. This stores the sequential characteristics/nature of the inputs and targets. In the case of a ViT used for classification, an input image is split into tokens, made up of separated, smaller 'patches' of the original image. Patches generate a positional embedding, based on the position of the patch within the entire image. The target values would be categorical labels, used to classify the input images.

These are converted into Keys, Queries, and Values, which are fed into multi-head attention modules, followed by small, linear feed-forward networks. An encoded representation of the original inputs is produced, which is fed into other network components (as well as target values). Finally, a softmax layer is applied to the outputs, giving the probabilities of each class being predicted.

Here is a diagram illustrating the main components of a ViT:
![(Dosovitskiy et al., 2021)](plots/Dosovitskiy_etal_2021_ViT_diagram.PNG)
*(Dosovitskiy et al., 2021)*

The overall architecture of this ViT was based on the ViT-S/16 model, as mentioned in the 2022 IEEE conference paper "Scaling Vision Transformers" (Zhai et al.).

### S/16 model details
The S/16 model variant of the ViT was chosen, as it was believed that it would provide a "good tradeoff" between performance and computational speed/efficiency (Beyer et al., 2022). It was believed that this would be the most appropriate, where hardware resources were partly limited.

Model specs (as specified in Zhai et al, 2022):
- **Patch size:** 16x16
- **Number of encoder blocks (depth):** 12
- **Dimensionality of patch embeddings and self-attention modules (width):** 384
- **Number of attention heads:** 6
- **Dimension of hidden MLP Linear layers:** 1536

The created model differs slightly from the original S/16 ViT model, including modifications as added by Beyere et al. (2022). These include 2-dimensional sinusoids used for positional embeddings, and the use of global average/mean pooling instead of using a class token.


## Examples
### Model inputs - ADNI dataset
The input images are 2D MRI slices of a patient's brain, taken from a Alzheimer's 
Disease Neuroimaging Initiative (ADNI) dataset. 
The images used with this model have been preprocessed.
For more specific details, please see the [ADNI dataset website.](https://adni.loni.usc.edu/)

The model takes 224x224 colour/RGB images as inputs, where each RGB channel contains
intensity values for each pixel in the range [0, 255].

ADNI images are cropped and resized from 256x240 to 224x224.

![Sample data from the ADNI dataset](plots/ADNI_sample_data.png)

Each image is also assigned a class label representing whether or not Alzheimer's 
disease was observed in the patient. These labels are "AD" (Alzheimer's Detected) 
and "NC" (Cognitive Normal). Within the model, these labels are transformed into
numerical categorical values (0 for AD, 1 for NC).

### Model outputs
A binary classification label is generated by the model, based on the input image.
A 0 value is returned if the AD (Alzheimer's Detected) is predicted, and a 1 value
is returned if predicting the NC (Cognitive Normal) class.


## Preprocessing

### Image resizing
Input images of size 256x240 were resized, then center cropped to a square
resolution of 224x224, which was used as the image input size for the model.
As the images were approximately centred (and all positioned similarly), 
processing them in this manner preserved the position of brains in the MRI slices,
whilst resizing them to dimensions that could be evenly downsampled multiple times
by the model network.

### Normalising the data
The files were loaded as RGB images. These contained 3 channels, in which each
channel represents an intensity value in the range [0, 255]. These values were
standardised such that the means and standard deviations were both changed to
0.5. This placed intensity values for each channel within the range [-1, 1]. 

### Data augmentation
No data augmentation was explicitly applied to the input data. However, all available
MRI image slices for each patient were used in the data set (20 slices per patient). As these slices are
distinct, but all map the same patient's brain (for the same classification), 
these slices may act similarly to 'augmented' data. This may result in the model being
more invariant to changes in the different MRI slices provided to it. In some contexts
where additional unseen data (potentially from different datasets) is tested, 
the model could potentially be more generalisable as a result.

### Train, validation, and test splits
The data was split into a training, validation, and test set. 
Training set data was used to train the model, with binary cross-entropy loss used
to evaluate its performance throughout the training process.

During training, the model performance was evaluated on the validation set at the final step of
each epoch. The relationship between training set loss and validation set loss was observed, to note the 
points of training in which the model was overfitting or underfitting. The most optimal 
length of time for training the model was manually selected, and the model was re-trained with a
different number of epochs. As such, validation set performance was used to perform tuning/selection of
a hyperparameter (the number of epochs).

The test set was used to evaluate the model performance on unseen data, quantified
by the accuracy metric.

#### Number of data points
The ADNI dataset contains 1526 patients (30520 MRI image slices).
The test set was composed of data points sampled from the 'test' directory of the ADNI dataset.
This set contained MRI image slices from 223 AD patients (4460 images) and
227 NC patients (4540 images), giving 450 patients (9000 images total). The test set
comprises of roughly 29-30% of the entire dataset.

Training and validation sets contained points sampled from the 'train' directory of this dataset,
which contains around 70-71% of the data.
80% of 'train' dir data (860 patients, 17200 images) was used in the training set, 
with 416 AD patients (8320 images) and 444 NC patients (8880 images).
20% (216 patients, 4320 images) of this data was used in the validation set - 
this contained 104 AD patients (2080 images) and 112 NC patients (2240 images).

#### Justification
The validation set was chosen to be approximately half the size of the test set.
It was considered more beneficial to quantify the model's performance on
a larger selection of unseen data (in the test set), than to utilise more of this
data for the purpose of hyperparameter tuning or training. When more data is moved to the
test set, the distribution of test data more accurately represents the 
characteristics of the entire dataset. 
The size of the training set was also not decreased in favour of the other sets used,
to allow for the model to train on an appropriate quantity of varying data points.

The split between the training and validation sets was stratified 
(attempting to roughly preserve the class proportions within each split set).
A stratified split can result in more effective training/useful testing. In saying
this, I don't believe that this has made a significant difference to this model
(as the class proportions are almost approximately equal).

#### Preventing data leakage
The training and validation set data appears to be independent from the test set data,
with no overlapping patient MRI images within both of the 'train' and 'test' data
directories.

To prevent data leakage within the train and validation sets (split from the 'train'
directory data), the MRI slices were grouped by patient, then the patients were 
shuffled and split between each set. After the split, data points (each MRI image 
slice) were shuffled within each set. This process ensured that the data was 
appropriately shuffled, whilst preventing images from one patient being allocated 
to both the train and validation set.


## Results:
### Training for 90 epochs:
To perform hyperparameter tuning for the number of epochs, a large number of epochs (90) was initially chosen. The results from the training and validation sets were saved and plotted:

![90 epochs - training vs. validation loss](plots/ViT_90epochs_loss.png)
Noticeably, the training and validation set loss appears to reach an optimal point between 40 and 60 epochs, then begins to fluctuate significantly at some points after this, despite reaching what appear to be the optimal loss values. 

Validation set accuracy (per-batch) approaches 100% around this training period:

![90 epochs - validation accuracy](plots/ViT_90epochs_validation_accuracy.png)

 These values could appear optimal on paper. However, the test set performance for the model trained for 90 epochs was ~60.32%, indicating that the model is overfitting significantly to the training set, and generalising poorly to the test set.

 ![90 epochs - confusion matrix on test set](plots/ViT_90epochs_test_confusion_matrix.png)
 
The confusion matrix for the model (based on the test set predictions) shows that
the model makes a significant number of incorrect predictions. Notably, the model predicts high quantities of False Positives (where AD/Alzheimer's Detected is the positive class), and a signficiantly smaller amount of False Negatives (where NC/Cognitive Normal is the negative class).

 To avoid overfitting the model, a model with a shorter training duration of 40 epochs was created and tested.

### Training for 40 epochs:

![40 epochs - training vs. validation loss](plots/ViT_40epochs_loss.png)

The training and validation loss, and the validation set accuracy indicate that the model reaches the most optimal loss values at the end of training, and this model likely suffers from less overfitting as a result. 

![40 epochs - validation accuracy](plots/ViT_40epochs_validation_accuracy.png)

However, the test set for this model performed only marginally better than the 90 epoch model, with a test accuracy of ~60.98%. This indicates that the model still poorly generalises to unseen data, and this can be seen in the model's confusion matrix:

![40 epochs - confusion matrix on test set](plots/ViT_40epochs_test_confusion_matrix.png)

Conversely to the 90 epoch model, many False Negatives and significantly fewer False Positives are predicted.
In some contexts, it may be preferable for a model to predict False Positives over False Negatives.
For medical circumstances, mistakenly predicting the existence of the condition in a healthy person may be preferred over not predicting the condition in a sick person, as high False Negative rates can prevent sick individuals from
receiving timely preventative treatment. Because of this, the 90 epochs model could be a more optimal choice of the two in some contexts. Even though test set accuracy is higher in the 40 epochs model, the difference in test set performance between the two is relatively minor.

A model trained on 30 epochs was tested, to see if the generalisability of the model could be increased further.

### Training for 30 epochs:

![30 epochs - training vs. validation loss](plots/ViT_loss.png) 

![30 epochs - validation accuracy](plots/ViT_validation_accuracy.png)

This model had an accuracy of ~60.37% on the test set - the performance of this model appeared to be somewhere inbetween the 90 epoch and 40 epoch models, although the difference in performance between these models is incredibly minor (less than 1% difference in test accuracy between any of these models).

![40 epochs - confusion matrix on test set](plots/ViT_test_confusion_matrix.png)

Similarly to the 40 epochs model, a much higher number of the misclassified images were False Negatives, and much less were false positives. 

As the test accuracy for this model is lower than the 40 epochs model, and this model also predicts high quantites of False Negatives, it would likely not be considered the most optimal model, unless the computational efficiency and faster training time of the model were of significant concern.


### Summary 
Depending on the context of the model's use, either the 90 epoch or 40 epoch model may be more contextually appropriate. If arbirarily higher test set performance is desired above anything, and faster/more efficient model training is also preferred, then the 40 epoch model would be the most ideal candidate of these three. If the model was required to predict more False Postives than False Negatives, then the 90 epoch model may be the more ideal choice for this. There may also be a more appropriate length of training time (between 40 and 90 epochs) that results in higher performing models than these three. Some of these models may also be the more optimal choice for saving training time and preferencing certain misclassifications over others.


## Possible improvements?
The model's performance is not ideal in its current state; the test set results illustrate
that the model is prone to overfitting, and does not generalise well to unseen data.

The model could be improved on or further experimented with, in multiple different ways:

### Data augmentation:
The majority of the images featured the brains positioned roughly in the middle,
with an approximate orientation. However, it was noted that the ADNI dataset would
occasionally contain MRI images that were rotated or positioned differently to others.
As the model would be less likely to see this data during the training process, 
it may be less invariant to minor rotations and positional changes. To improve the
model performance and make the model more generalisable, it could be regularised by augmenting the training set data. Images could be duplicated within the training set, then augmented with minor rotations and positional translations. Beyer et al. (2022) noted
that ViT models experienced improved performance on baseline image processing datasets
with the use of similar data augmentation techniques.

### Changing RGB images to greyscale:
The input data is MRI images, in which each pixel of the image is a "greyscale value 
that ranges from 0 (pure black) to 255 (pure white)" (Gerber & Peterson, 2008). 
Because of this, loading the input data as RGB images results in superfluous 
information (the intensity values in each channel are identical). 

Converting the image to a single-channel, greyscale format would likely improve
the computational efficiency of the model during training and inference.
It's also possible that the model could achieve more ideal performance by training on data points with less superfluous data.

### More hyperparameter tuning:
Whilst the number of epochs/length of training was examined with a validation set,
many other model hyperparameters could additionally be tuned. For example, other
model configurations listed in the Zhai et al. (2022) paper could also be tested on the ADNI dataset, and their performance compared with the S/16 model. 
This paper also suggests using either a constant or a reciprocal square-root 
learning rate scheduler, to prevent the learning rate from decaying too quickly during training.
This scheduler would include a "warmup" and a "cooldown" period.
Currently, the PyTorch OneCycleLR scheduler is used to "warmup", then "cooldown" the 
learning rate in a non-linear fashion. It's possible that a constant or inverse
square-root scheduler may encourage the model to train more effectively, as the learning rate at some points will decay at a slower rate.

### Other regularisation techniques:
As the model appeared to often suffer from overfitting, some other model-based regularisation techniques such as network dropout could improve the network's generalisability. As well as this, the hyperparameter tuning of the training time (number of epochs) could be utilised to perform early stopping on the model training process, once the loss values have converged to within a given
threshold.


## References
- Alzheimer's Disease Neuroimaging Initiative. (2017). *ADNI | Alzheimer's Disease Neuroimaging Initiative.* https://adni.loni.usc.edu/

- Beyer, L., Zhai, X., Kolesnikov, A. (2022). *Better plain ViT baselines for ImageNet-1k.* https://arxiv.org/abs/2205.01580

- Doshi, K. (2021, January 17). *Transformers explained visually (Part 3): Multi-head attention, deep dive.* https://towardsdatascience.com/
transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853

-   Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G.,  Gelly, S., Uszkoreit, J., Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. *International Conference on Learning Representations.* https://arxiv.org/pdf/2010.11929.pdf

- Gerber, A. J., & Peterson, B. S. (2008). What is an image? *Journal of the American Academy of Child and Adolescent Psychiatry*, 47(3), 245–248. https://doi.org/10.1097/CHI.0b013e318161e509

- Raschka, S. (2022, June 12). *Taking Datasets, DataLoaders, and PyTorch's new DataPipes for a spin.* https://sebastianraschka.com/blog/2022/datapipes.html#DataPipesforDatasetsWithImagesandCSVs

- Zhai, X. Kolesnikov, A., Houlsby, N., Beyer, L. (2022). Scaling Vision Transformers. *Institute of Electrical and Electronics Engineers.* https://ieeexplore.ieee.org/document/9880094

