{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338da719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports Here\n",
    "\"\"\"\n",
    "\"\"\"numpy and torch\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\"\"\"PIL\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"torchvision and utils\"\"\"\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\"\"\"os\"\"\"\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65011ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLoading data from local file\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loading data from local file\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "206e485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Assumes images have pixel values in range [0,255]\"\"\"\n",
    "def getImages(trainDIRs, testDIRS):\n",
    "    \"\"\"Get image to tensor\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor()\n",
    "    ])\n",
    "    \"\"\"Loading data into arrays\"\"\"\n",
    "    xtrain, xtrain, xtest, ytest = [], [], [], []\n",
    "    \"\"\"training data\"\"\"\n",
    "    size = [0, 0]\n",
    "    for i, DIR in enumerate(trainDIRs):\n",
    "        px = []\n",
    "        j = 0\n",
    "        for filename in sorted(os.listdir(DIR)):\n",
    "            f = os.path.join(DIR, filename)\n",
    "            img = Image.open(f)\n",
    "            tensor = transform(img).float()\n",
    "            tensor.require_grad = True\n",
    "            px.append(tensor/255)\n",
    "            j  = (j+1) % 20\n",
    "            if j == 0:\n",
    "                xtrain.append(torch.stack(px))\n",
    "                px = []\n",
    "                size[i] += 1\n",
    "    xtrain = torch.stack(xtrain)\n",
    "    ytrain = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))\n",
    "        \n",
    "    \"\"\"testing data\"\"\"\n",
    "    size = [0, 0]\n",
    "    for i, DIR in enumerate(testDIRs):\n",
    "        px = []\n",
    "        j = 0\n",
    "        for filename in sorted(os.listdir(DIR)):\n",
    "            f = os.path.join(DIR, filename)\n",
    "            img = Image.open(f)\n",
    "            tensor = transform(img).float()\n",
    "            tensor.require_grad = True\n",
    "            px.append(tensor/255)\n",
    "            if j == 0:\n",
    "                xtest.append(torch.stack(px))\n",
    "                px = []\n",
    "                size[i] += 1\n",
    "    xtest = torch.stack(xtest)\n",
    "    ytest = torch.from_numpy(np.concatenate((np.ones(size[0]), np.zeros(size[1])), axis=0))\n",
    "    return xtrain, ytrain, xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3c45c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1076, 20, 1, 240, 256])\n",
      "torch.Size([9000, 1, 1, 240, 256])\n"
     ]
    }
   ],
   "source": [
    "trainDIRs = ['../../../AD_NC/train/AD/', '../../../AD_NC/train/NC']\n",
    "testDIRs = ['../../../AD_NC/test/AD/', '../../../AD_NC/test/NC']\n",
    "xtrain, ytrain, xtest, ytest = getImages(trainDIRs, testDIRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "292100c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPatches(imgs, patchsize):\n",
    "    (N, M, C, W, H) = imgs.shape\n",
    "    (wsize, hsize) = patchsize\n",
    "    \"\"\"check for errors with sizing\"\"\"\n",
    "    if (W % wsize != 0) or (H % hsize != 0):\n",
    "        raise Exception(\"patchsize is not appropriate\")\n",
    "    if (C != C) or (H != H):\n",
    "        raise Exception(\"given sizes do not match\")\n",
    "    size = (N, M, C, W // wsize, wsize, H // hsize, hsize)\n",
    "    perm = (0, 1, 3, 5, 2, 4, 6) #bring col, row index of patch to front\n",
    "    flat = (2, 3) #flatten (col, row) index into col*row entry index for patches\n",
    "    imgs = imgs.reshape(size).permute(perm).flatten(*flat)\n",
    "    return imgs #in format Nimgs, Npatches, C, Wpatch, Hpatch\n",
    "    \n",
    "def flattenPatches(imgs): #takes input (N, M, Npatches, C, W, H) returns (N, M*Npatches, C*W*H)\n",
    "    return imgs.flatten(3, 5).flatten(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0897522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDataloader\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataloader\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c80732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X, self.y = X, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        else:\n",
    "            return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea41eef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1076, 20, 1, 240, 256])\n",
      "torch.Size([9000, 1, 1, 240, 256])\n"
     ]
    }
   ],
   "source": [
    "trainDIRs = ['../../../AD_NC/train/AD/', '../../../AD_NC/train/NC']\n",
    "testDIRs = ['../../../AD_NC/test/AD/', '../../../AD_NC/test/NC']\n",
    "xtrain, ytrain, xtest, ytest = getImages(trainDIRs, testDIRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f077f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = flattenPatches(createPatches(xtrain, (16,16)))\n",
    "xtest = flattenPatches(createPatches(xtest, (16,16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a02e05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainloader(batchsize=16):\n",
    "    return DataLoader(DatasetWrapper(xtrain, ytrain), batchsize=batchsize, shuffle=True)\n",
    "\n",
    "def testloader():\n",
    "    return DataLoader(DatasetWrapper(xtest, ytest), batchsize=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18d6ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainshape():\n",
    "    return xtrain.shape\n",
    "\n",
    "def testshape():\n",
    "    return xtest.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
