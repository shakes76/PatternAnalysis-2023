{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMOX2gKBKyi2iA+b1jvHAIk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uqmshawn/uqmshawn-4-7-1-8-4-5-3-0-r/blob/main/47184530.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "C1oCBoLsedJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class BrainSlicesDataset(Dataset):\n",
        "    def __init__(self, image_slices):\n",
        "        self.image_slices = image_slices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_slices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.image_slices[idx]\n",
        "\n",
        "        # Ensure the image has a channel dimension\n",
        "        if len(image.shape) == 2:  # If the image is of shape [H, W]\n",
        "            image = torch.unsqueeze(image, 0)  # Convert it to [1, H, W]\n",
        "\n",
        "        return image\n",
        "\n",
        "def get_image_slices():\n",
        "    zip_path = \"/content/drive/MyDrive/Colab_Notebooks_Course/image_process/A3/testgans/GAN_Dataset.zip\"\n",
        "    extraction_path = \"/content/GAN_Dataset\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_path)\n",
        "\n",
        "    parent_dir = \"/content/GAN_Dataset\"\n",
        "    train_path = os.path.join(parent_dir, \"keras_png_slices_train\")\n",
        "    test_path = os.path.join(parent_dir, \"keras_png_slices_test\")\n",
        "    val_path = os.path.join(parent_dir, \"keras_png_slices_validate\")\n",
        "\n",
        "    def load_images_from_folder(folder_path):\n",
        "            images = []\n",
        "            for filename in os.listdir(folder_path):\n",
        "                img = Image.open(os.path.join(folder_path, filename)).convert('L').resize((128, 128))\n",
        "                if img is not None:\n",
        "                    images.append(torch.tensor(np.array(img, dtype=np.float32)))\n",
        "            return torch.stack(images)\n",
        "\n",
        "    train_images = load_images_from_folder(train_path)\n",
        "    test_images = load_images_from_folder(test_path)\n",
        "    validate_images = load_images_from_folder(val_path)\n",
        "\n",
        "    # Print statements to understand the data\n",
        "    print(f\"Total train images: {len(train_images)}\")\n",
        "    print(f\"Shape of a single train image: {train_images[0].shape}\")\n",
        "    print(f\"Total test images: {len(test_images)}\")\n",
        "    print(f\"Shape of a single test image: {test_images[0].shape}\")\n",
        "    print(f\"Total validation images: {len(validate_images)}\")\n",
        "    print(f\"Shape of a single validation image: {validate_images[0].shape}\")\n",
        "\n",
        "    return train_images, test_images, validate_images\n",
        "\n",
        "# Call the function to see the print outputs\n",
        "get_image_slices()"
      ],
      "metadata": {
        "id": "4cFXg40HfDzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f4c4c1-54a9-4dbc-8c03-59f5466670a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total train images: 9664\n",
            "Shape of a single train image: torch.Size([128, 128])\n",
            "Total test images: 544\n",
            "Shape of a single test image: torch.Size([128, 128])\n",
            "Total validation images: 1120\n",
            "Shape of a single validation image: torch.Size([128, 128])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
              " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
              " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definitions: VectorQuantizer, Encoder, Decoder, VQVAE, PixelCNN, etc.\n",
        "\n",
        "# VectorQuantizer Layer\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, beta=0.25):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.beta = beta\n",
        "\n",
        "        # Learnable codebook embeddings\n",
        "        self.embeddings = nn.Parameter(torch.randn(embedding_dim, num_embeddings))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape input tensor to [batch_size, num_channels, height, width]\n",
        "        z_e_x = x.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        # Flatten the input tensor to [batch_size * height * width, embedding_dim]\n",
        "        z_e_x_ = z_e_x.view(-1, self.embedding_dim)\n",
        "\n",
        "        # Calculate distances between input vectors and codebook vectors\n",
        "        distances = (torch.sum(z_e_x_**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self.embeddings**2, dim=0)\n",
        "                    - 2 * torch.matmul(z_e_x_, self.embeddings))\n",
        "\n",
        "        # Find the index of the nearest codebook vector for each input vector\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "\n",
        "        # Create a one-hot encoding based on the indices\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings).to(x.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "         # Reshape encoding_indices to match the original input shape\n",
        "        encoding_indices = encoding_indices.view(*z_e_x.shape[:-1])\n",
        "\n",
        "        # Compute the quantized representation using the codebook\n",
        "        quantized = torch.matmul(encodings, self.embeddings.t()).view(*z_e_x.shape)\n",
        "\n",
        "        # Compute loss components\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), z_e_x)\n",
        "        q_latent_loss = F.mse_loss(quantized, z_e_x.detach())\n",
        "\n",
        "        # Calculate the total loss as a combination of the two losses\n",
        "        loss = q_latent_loss + self.beta * e_latent_loss\n",
        "\n",
        "        # Ensure that quantized is used in the gradient computation\n",
        "        quantized = z_e_x + (quantized - z_e_x).detach()\n",
        "\n",
        "        # Compute the perplexity of the encoding distribution\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # Return the loss, quantized tensor, perplexity, and encoding indices\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encoding_indices\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels // 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels // 2, embedding_dim, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(input_channels, hidden_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels, hidden_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels // 2, 1, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "# VQVAE\n",
        "# VQVAETrainer\n",
        "# PixelConvLayer & PixelCNN"
      ],
      "metadata": {
        "id": "7hS9rkMYiCvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Functions: train_vqvae, train_pixelcnn, etc.\n",
        "\n",
        "# train_vqvae\n",
        "# train_pixelcn"
      ],
      "metadata": {
        "id": "VinNRMXdfGcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_reconstructions(originals, reconstructions, num_samples=3):\n",
        "    # ... [Code for visualizing reconstructions]\n",
        "\n",
        "def visualize_samples(samples, num_samples=3):\n",
        "    # ... [Code for visualizing samples]"
      ],
      "metadata": {
        "id": "NyKo37QboLT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # ... [Main function code]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "imgplgzCoNq6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}