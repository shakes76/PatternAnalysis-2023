{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN8rI8GEcbZpCpRbovbCTym",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uqmshawn/uqmshawn-4-7-1-8-4-5-3-0-r/blob/main/47184530.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "C1oCBoLsedJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class BrainSlicesDataset(Dataset):\n",
        "    def __init__(self, image_slices):\n",
        "        self.image_slices = image_slices\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_slices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.image_slices[idx]\n",
        "\n",
        "        # Ensure the image has a channel dimension\n",
        "        if len(image.shape) == 2:  # If the image is of shape [H, W]\n",
        "            image = torch.unsqueeze(image, 0)  # Convert it to [1, H, W]\n",
        "\n",
        "        return image\n",
        "\n",
        "def get_image_slices():\n",
        "    zip_path = \"/content/drive/MyDrive/Colab_Notebooks_Course/image_process/A3/testgans/GAN_Dataset.zip\"\n",
        "    extraction_path = \"/content/GAN_Dataset\"\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_path)\n",
        "\n",
        "    parent_dir = \"/content/GAN_Dataset\"\n",
        "    train_path = os.path.join(parent_dir, \"keras_png_slices_train\")\n",
        "    test_path = os.path.join(parent_dir, \"keras_png_slices_test\")\n",
        "    val_path = os.path.join(parent_dir, \"keras_png_slices_validate\")\n",
        "\n",
        "    def load_images_from_folder(folder_path):\n",
        "            images = []\n",
        "            for filename in os.listdir(folder_path):\n",
        "                img = Image.open(os.path.join(folder_path, filename)).convert('L').resize((128, 128))\n",
        "                if img is not None:\n",
        "                    images.append(torch.tensor(np.array(img, dtype=np.float32)))\n",
        "            return torch.stack(images)\n",
        "\n",
        "    train_images = load_images_from_folder(train_path)\n",
        "    test_images = load_images_from_folder(test_path)\n",
        "    validate_images = load_images_from_folder(val_path)\n",
        "\n",
        "    # Print statements to understand the data\n",
        "    print(f\"Total train images: {len(train_images)}\")\n",
        "    print(f\"Shape of a single train image: {train_images[0].shape}\")\n",
        "    print(f\"Total test images: {len(test_images)}\")\n",
        "    print(f\"Shape of a single test image: {test_images[0].shape}\")\n",
        "    print(f\"Total validation images: {len(validate_images)}\")\n",
        "    print(f\"Shape of a single validation image: {validate_images[0].shape}\")\n",
        "\n",
        "    return train_images, test_images, validate_images\n",
        "\n",
        "# Call the function to see the print outputs\n",
        "get_image_slices()"
      ],
      "metadata": {
        "id": "4cFXg40HfDzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39f4c4c1-54a9-4dbc-8c03-59f5466670a0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Total train images: 9664\n",
            "Shape of a single train image: torch.Size([128, 128])\n",
            "Total test images: 544\n",
            "Shape of a single test image: torch.Size([128, 128])\n",
            "Total validation images: 1120\n",
            "Shape of a single validation image: torch.Size([128, 128])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
              " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
              " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              " \n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definitions: VectorQuantizer, Encoder, Decoder, VQVAE, PixelCNN, etc.\n",
        "\n",
        "# VectorQuantizer Layer\n",
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, beta=0.25):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.beta = beta\n",
        "        self.embeddings = nn.Parameter(torch.randn(embedding_dim, num_embeddings))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate distances and indices\n",
        "        z_e_x = x.permute(0, 2, 3, 1).contiguous()\n",
        "        z_e_x_ = z_e_x.view(-1, self.embedding_dim)\n",
        "        distances = (torch.sum(z_e_x_**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self.embeddings**2, dim=0)\n",
        "                    - 2 * torch.matmul(z_e_x_, self.embeddings))\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings).to(x.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "        encoding_indices = encoding_indices.view(*z_e_x.shape[:-1])\n",
        "        quantized = torch.matmul(encodings, self.embeddings.t()).view(*z_e_x.shape)\n",
        "        # Compute loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), z_e_x)\n",
        "        q_latent_loss = F.mse_loss(quantized, z_e_x.detach())\n",
        "        loss = q_latent_loss + self.beta * e_latent_loss\n",
        "        quantized = z_e_x + (quantized - z_e_x).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encoding_indices\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, hidden_channels, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels, hidden_channels // 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_channels // 2, embedding_dim, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(input_channels, hidden_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels, hidden_channels // 2, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(hidden_channels // 2, 1, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(x)\n",
        "\n",
        "# VQVAE\n",
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, num_embeddings, embedding_dim):\n",
        "        super(VQVAE, self).__init__()\n",
        "        self.encoder = Encoder(input_channels, hidden_channels, embedding_dim)\n",
        "        self.quantize = VectorQuantizer(num_embeddings, embedding_dim)\n",
        "        self.decoder = Decoder(embedding_dim, hidden_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        loss, quantized, perplexity, _ = self.quantize(z)\n",
        "        x_recon = self.decoder(quantized)\n",
        "        return loss, x_recon, perplexity\n",
        "\n",
        "# VQVAETrainer\n",
        "class VQVAETrainer(nn.Module):\n",
        "    def __init__(self, train_variance, input_channels, hidden_channels, num_embeddings, embedding_dim):\n",
        "        super(VQVAETrainer, self).__init__()\n",
        "        self.train_variance = train_variance\n",
        "        self.vqvae = VQVAE(input_channels, hidden_channels, num_embeddings, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        vq_loss, x_recon, perplexity = self.vqvae(x)\n",
        "        recon_loss = F.mse_loss(x_recon, x) / self.train_variance\n",
        "        loss = recon_loss + vq_loss\n",
        "        return x_recon, perplexity, loss\n",
        "\n",
        "# PixelConvLayer & PixelCNN\n",
        "class PixelConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, mask_type, **kwargs):\n",
        "        super(PixelConvLayer, self).__init__()\n",
        "        self.mask_type = mask_type\n",
        "        self.padding = (kernel_size - 1) // 2\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, **kwargs, padding=self.padding)\n",
        "        self.mask = self.conv.weight.data.clone()\n",
        "        self.create_mask()\n",
        "    def forward(self, x):\n",
        "        self.conv.weight.data *= self.mask\n",
        "        return self.conv(x)\n",
        "    def create_mask(self):\n",
        "        _, _, H, W = self.conv.weight.size()\n",
        "        self.mask.fill_(1)\n",
        "        self.mask[:, :, H // 2, W // 2 + (self.mask_type == 'A'):] = 0\n",
        "        self.mask[:, :, H // 2 + 1:] = 0\n",
        "\n",
        "class PixelCNN(nn.Module):\n",
        "    def __init__(self, input_shape, num_embeddings, embedding_dim):\n",
        "        super(PixelCNN, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(PixelConvLayer(input_shape[0], embedding_dim, 7, mask_type='A'))\n",
        "        for _ in range(5):\n",
        "            self.layers.append(PixelConvLayer(embedding_dim, embedding_dim, 7, mask_type='B'))\n",
        "        self.layers.append(nn.Conv2d(embedding_dim, num_embeddings, 1))\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "7hS9rkMYiCvW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vqvae(vqvae, train_loader, num_epochs, learning_rate, test_samples, recon_losses, vq_losses, perplexities):\n",
        "    optimizer = optim.Adam(vqvae.parameters(), lr=learning_rate)\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, images in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            x_recon, perplexity, loss = vqvae(images)\n",
        "\n",
        "            # Record the losses and perplexity\n",
        "            recon_loss_value = F.mse_loss(x_recon, images) / vqvae.train_variance\n",
        "            vq_loss_value = loss - recon_loss_value\n",
        "            recon_losses.append(recon_loss_value.item())\n",
        "            vq_losses.append(vq_loss_value.item())\n",
        "            perplexities.append(perplexity.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Visualization at the end of each epoch\n",
        "        with torch.no_grad():\n",
        "            reconstructions, _, _ = vqvae(test_samples)\n",
        "            visualize_reconstructions(test_samples.cpu(), reconstructions.cpu())\n",
        "\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(recon_losses, label='Reconstruction Loss')\n",
        "    plt.plot(vq_losses, label='VQ Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Losses over Training')\n",
        "    plt.xlabel('Training Iterations')\n",
        "    plt.ylabel('Loss Value')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(perplexities)\n",
        "    plt.title('Perplexity over Training')\n",
        "    plt.xlabel('Training Iterations')\n",
        "    plt.ylabel('Perplexity')\n",
        "    plt.show()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, _, _, encoding_indices = vqvae.vqvae.quantize(vqvae.vqvae.encoder(test_samples))\n",
        "        encoding_indices = encoding_indices.flatten().cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist(encoding_indices, bins=np.arange(vqvae.vqvae.quantize.num_embeddings+1)-0.5, rwidth=0.8)\n",
        "    plt.title('Histogram of Encoding Indices')\n",
        "    plt.xlabel('Encoding Index')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xticks(np.arange(vqvae.vqvae.quantize.num_embeddings))\n",
        "    plt.show()\n",
        "\n",
        "def train_pixelcnn(pixelcnn, train_loader, num_epochs, learning_rate):\n",
        "    optimizer = optim.Adam(pixelcnn.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        for images in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            logits = pixelcnn(images)\n",
        "            loss = criterion(logits, images.squeeze(1).long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "VinNRMXdfGcr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_reconstructions(originals, reconstructions, num_samples=3):\n",
        "    # Visualization function for showing original vs reconstructed images\n",
        "    for i in range(num_samples):\n",
        "        fig, axs = plt.subplots(1, 2)\n",
        "        axs[0].imshow(originals[i, 0].detach().numpy(), cmap='gray')\n",
        "        axs[0].set_title(\"Original\")\n",
        "        axs[1].imshow(reconstructions[i, 0].detach().numpy(), cmap='gray')\n",
        "        axs[1].set_title(\"Reconstruction\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def visualize_samples(samples, num_samples=3):\n",
        "    # Visualization function for generated samples\n",
        "    for i in range(num_samples):\n",
        "        plt.imshow(samples[i, 0].detach().numpy(), cmap='gray')\n",
        "        plt.title(\"Generated Sample\")\n",
        "        plt.show()\n",
        "\n",
        "def visualize_pixelcnn_generation_batch(pixelcnn, batch_size, img_size=(1, 128, 128)):\n",
        "    # Create a batch of empty images\n",
        "    samples = torch.zeros(batch_size, *img_size).to(device)\n",
        "\n",
        "    # Generate images pixel by pixel\n",
        "    for i in range(img_size[1]):\n",
        "        for j in range(img_size[2]):\n",
        "            out = pixelcnn(samples)\n",
        "            probs = F.softmax(out[:, :, i, j], dim=1)\n",
        "            for b in range(batch_size):\n",
        "                samples[b, :, i, j] = torch.multinomial(probs[b], 1).float() / 255.0\n",
        "\n",
        "    # Display the generated images\n",
        "    for b in range(batch_size):\n",
        "        plt.imshow(samples[b, 0].cpu().detach().numpy(), cmap='gray')\n",
        "        plt.title(f\"PixelCNN Generated Sample {b+1}\")\n",
        "        plt.show()\n",
        "\n",
        "def compare_original_and_generated(original, pixelcnn, img_size=(1, 128, 128)):\n",
        "    # Generate an image using PixelCNN\n",
        "    generated = torch.zeros(img_size).to(device)\n",
        "    for i in range(img_size[1]):\n",
        "        for j in range(img_size[2]):\n",
        "            out = pixelcnn(generated)\n",
        "            probs = F.softmax(out[:, :, i, j], dim=1)\n",
        "            generated[:, :, i, j] = torch.multinomial(probs, 1).float() / 255.0\n",
        "\n",
        "    # Visualization\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axs[0].imshow(original[0, 0].cpu().detach().numpy(), cmap='gray')\n",
        "    axs[0].set_title(\"Original\")\n",
        "    axs[1].imshow(generated[0, 0].cpu().detach().numpy(), cmap='gray')\n",
        "    axs[1].set_title(\"PixelCNN Generated\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "NyKo37QboLT9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    recon_losses = []\n",
        "    vq_losses = []\n",
        "    perplexities = []\n",
        "\n",
        "\n",
        "    train_images, test_images, _ = get_image_slices()\n",
        "    dataset = BrainSlicesDataset(train_images)\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "imgplgzCoNq6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}